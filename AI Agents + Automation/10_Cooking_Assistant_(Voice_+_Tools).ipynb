{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaZ1cwz6Chs3Hr/3bKkO9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammadhsiavash/DeepL-Training/blob/main/AI%20Agents%20%2B%20Automation/10_Cooking_Assistant_(Voice_%2B_Tools).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build a multimodal cooking assistant that:**\n",
        "1. Accepts voice commands\n",
        "2. Uses tools to:\n",
        "Fetch recipes\n",
        "Read steps aloud\n",
        "Convert units (e.g., cups to grams)\n",
        "3. Remembers where you are in the recipe\n",
        "\n",
        "**Agent Patern:** Voice Agent + Tool Routing\n",
        "\n",
        "**Tools:**\n",
        "\n",
        "Speech-to-Text (STT): e.g., whisper or speech_recognition\n",
        "\n",
        "Recipe Fetcher (via Spoonacular API or local JSON)\n",
        "\n",
        "Unit Converter\n",
        "\n",
        "Step Tracker (short-term memory)\n"
      ],
      "metadata": {
        "id": "0YIQtX_9Vk7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Libraries"
      ],
      "metadata": {
        "id": "9UDF3iIQWcqk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTDBxX-jVcAd",
        "outputId": "c98d9ccf-2a50-4c08-c4ac-2c340d80dd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyttsx3\n",
            "  Downloading pyttsx3-2.99-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting speechrecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from speechrecognition) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading pyttsx3-2.99-py3-none-any.whl (32 kB)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=717b250f9ff684c0f2d382839b81931f53a7ceee223ee8a4859009fc653700c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: pyttsx3, speechrecognition, openai-whisper\n",
            "Successfully installed openai-whisper-20250625 pyttsx3-2.99 speechrecognition-3.14.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper pyttsx3 speechrecognition requests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "563e8973",
        "outputId": "8c0b6bc3-c8fc-4d85-a885-50b737ad47af"
      },
      "source": [
        "!sudo apt-get update && sudo apt-get install espeak-ng"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.4 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,002 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,243 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,790 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Fetched 24.3 MB in 4s (6,585 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 4,526 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 4,526 kB in 2s (2,153 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Voice Input and Output"
      ],
      "metadata": {
        "id": "FGRinKf1WmdS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b40f075",
        "outputId": "10c6f7fd-35d8-4581-d426-b02a336b07cb"
      },
      "source": [
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import whisper\n",
        "\n",
        "# Initialize mic and text-to-speech\n",
        "r = sr.Recognizer()\n",
        "tts = pyttsx3.init()\n",
        "model = whisper.load_model(\"base\") # Load the Whisper model\n",
        "\n",
        "def listen():\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"🎤 Listening...\")\n",
        "        audio = r.listen(source)\n",
        "\n",
        "    try:\n",
        "        # Save the audio to a temporary file\n",
        "        with open(\"temp_audio.wav\", \"wb\") as f:\n",
        "            f.write(audio.get_wav_data())\n",
        "\n",
        "        # Use Whisper to transcribe the audio\n",
        "        result = model.transcribe(\"temp_audio.wav\")\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during transcription: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def speak(text):\n",
        "    print(\"🗣 \" + text)\n",
        "    tts.say(text)\n",
        "    tts.runAndWait()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 46.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Tool Set"
      ],
      "metadata": {
        "id": "qNx14-X3XLnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A. Recipe Fetcher (Simulated)\n",
        "recipe = {\n",
        "  \"title\": \"Pancakes\",\n",
        "  \"ingredients\": [\"1 cup flour\", \"2 eggs\", \"1 cup milk\", \"1 tsp baking powder\"],\n",
        "  \"steps\": [\n",
        "    \"Mix all the dry ingredients in a bowl.\",\n",
        "    \"Add eggs and milk. Stir until smooth.\",\n",
        "    \"Heat a skillet and pour 1/4 cup batter.\",\n",
        "    \"Flip when bubbles form. Cook until golden.\"\n",
        "  ]\n",
        "}\n",
        "# B. Unit Converter\n",
        "def convert_units(ingredient):\n",
        "  conversions = {\n",
        "    \"1 cup flour\": \"120 grams of flour\",\n",
        "    \"1 cup milk\": \"240 ml of milk\"\n",
        "  }\n",
        "  return conversions.get(ingredient.lower(), \"Conversion not found.\")"
      ],
      "metadata": {
        "id": "eql1OMldXRCc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Cooking Agent with Step Memory"
      ],
      "metadata": {
        "id": "3Rkk-3dGYK_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_step = 0\n",
        "def handle_command(command):\n",
        "  global current_step\n",
        "  if \"start\" in command:\n",
        "    speak(f\"Let's cook {recipe['title']}. Say 'next' when you're ready.\")\n",
        "  elif \"ingredients\" in command:\n",
        "    speak(\", \".join(recipe['ingredients']))\n",
        "  elif \"convert\" in command:\n",
        "    for item in recipe['ingredients']:\n",
        "      speak(convert_units(item))\n",
        "  elif \"next\" in command:\n",
        "    if current_step < len(recipe['steps']):\n",
        "      speak(recipe['steps'][current_step])\n",
        "      current_step += 1\n",
        "    else:\n",
        "      speak(\"You're done! Enjoy your meal.\")\n",
        "  elif \"repeat\" in command:\n",
        "    if current_step > 0:\n",
        "      speak(recipe['steps'][current_step - 1])\n",
        "  elif \"back\" in command:\n",
        "    current_step = max(0, current_step - 1)\n",
        "    speak(recipe['steps'][current_step])"
      ],
      "metadata": {
        "id": "QZ-QcrHDYHly"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Assistant"
      ],
      "metadata": {
        "id": "VT7KOCLtY6XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speak(\"Hi! I’m your Cooking Assistant. Say 'start' to begin.\")\n",
        "failed_attempts = 0\n",
        "while True:\n",
        "  try:\n",
        "    user_command = listen().lower()\n",
        "    if user_command:\n",
        "        handle_command(user_command)\n",
        "        failed_attempts = 0  # Reset failed attempts on successful command\n",
        "    else:\n",
        "        failed_attempts += 1\n",
        "        if failed_attempts >= 5: # Exit after 5 failed attempts\n",
        "            speak(\"I'm having trouble understanding. Exiting.\")\n",
        "            break\n",
        "        speak(\"Sorry, I didn't catch that.\")\n",
        "\n",
        "    if \"exit\" in user_command:\n",
        "      speak(\"Goodbye, and happy cooking!\")\n",
        "      break\n",
        "  except Exception as e:\n",
        "    failed_attempts += 1\n",
        "    if failed_attempts >= 5: # Exit after 5 failed attempts due to exception\n",
        "        speak(\"An error occurred. Exiting.\")\n",
        "        break\n",
        "    speak(\"Sorry, I didn't catch that.\")\n",
        "    print(f\"Error: {e}\") # Print the error for debugging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JezjaIRaYqNF",
        "outputId": "9dff734c-dc09-4cb0-8a8e-d52af8ef3458"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗣 Hi! I’m your Cooking Assistant. Say 'start' to begin.\n",
            "🗣 Sorry, I didn't catch that.\n",
            "Error: Could not find PyAudio; check installation\n",
            "🗣 Sorry, I didn't catch that.\n",
            "Error: Could not find PyAudio; check installation\n",
            "🗣 Sorry, I didn't catch that.\n",
            "Error: Could not find PyAudio; check installation\n",
            "🗣 Sorry, I didn't catch that.\n",
            "Error: Could not find PyAudio; check installation\n",
            "🗣 An error occurred. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c381ee3e"
      },
      "source": [
        "# Task\n",
        "Modify the code in the selected cell to use text input instead of voice input for simulating voice commands and add a markdown cell explaining why this change was made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "939efb56"
      },
      "source": [
        "## Explain the reason for simulating voice input\n",
        "\n",
        "### Subtask:\n",
        "Add a markdown cell explaining why we are modifying the code to use text input instead of actual voice input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637f4af2"
      },
      "source": [
        "## Explain the reason for simulating voice input\n",
        "\n",
        "### Subtask:\n",
        "Add a markdown cell explaining why we are modifying the code to use text input instead of actual voice input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bec3487"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a markdown cell to explain the reason for using text input simulation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8b6f3e50",
        "outputId": "aeed9801-eed2-4b99-be0c-9ce5ff737329"
      },
      "source": [
        "# Simulating Voice Input with Text\n",
        "\n",
        "\"In this notebook environment, capturing live audio input can be challenging and may require specific configurations or dependencies not always available. To facilitate testing and demonstration of the cooking assistant's logic and tool interactions, we will simulate voice commands by using text input instead of actual microphone input. This allows us to easily test different commands and observe the assistant's responses.\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In this notebook environment, capturing live audio input can be challenging and may require specific configurations or dependencies not always available. To facilitate testing and demonstration of the cooking assistant's logic and tool interactions, we will simulate voice commands by using text input instead of actual microphone input. This allows us to easily test different commands and observe the assistant's responses.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608a17fb"
      },
      "source": [
        "## Modify the `listen` function\n",
        "\n",
        "### Subtask:\n",
        "Update the `listen` function to take text input from the user instead of capturing audio from the microphone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd40c5d5"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to modify the `listen` function to take text input instead of audio input. This involves removing the microphone and whisper-related code and using the `input()` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8df9599"
      },
      "source": [
        "# Initialize mic and text-to-speech\n",
        "# r = sr.Recognizer() # No longer needed for text input\n",
        "tts = pyttsx3.init()\n",
        "# model = whisper.load_model(\"base\") # No longer needed for text input\n",
        "\n",
        "def listen():\n",
        "    try:\n",
        "        user_input = input(\"Enter command: \") # Get text input from user\n",
        "        return user_input\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text input: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def speak(text):\n",
        "    print(\"🗣 \" + text)\n",
        "    tts.say(text)\n",
        "    tts.runAndWait()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c665f76"
      },
      "source": [
        "## Test the simulated voice input\n",
        "\n",
        "### Subtask:\n",
        "Run the main loop to test if the assistant responds to the simulated text commands.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97bfc58d"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the code cell to run the main loop and test the assistant with simulated text commands.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84458abb",
        "outputId": "f29ef9b5-b162-47cf-b8e0-06086f30bcff"
      },
      "source": [
        "speak(\"Hi! I’m your Cooking Assistant. Say 'start' to begin.\")\n",
        "failed_attempts = 0\n",
        "while True:\n",
        "  try:\n",
        "    user_command = listen().lower()\n",
        "    if user_command:\n",
        "        handle_command(user_command)\n",
        "        failed_attempts = 0  # Reset failed attempts on successful command\n",
        "    else:\n",
        "        failed_attempts += 1\n",
        "        if failed_attempts >= 5: # Exit after 5 failed attempts\n",
        "            speak(\"I'm having trouble understanding. Exiting.\")\n",
        "            break\n",
        "        speak(\"Sorry, I didn't catch that.\")\n",
        "\n",
        "    if \"exit\" in user_command:\n",
        "      speak(\"Goodbye, and happy cooking!\")\n",
        "      break\n",
        "  except Exception as e:\n",
        "    failed_attempts += 1\n",
        "    if failed_attempts >= 5: # Exit after 5 failed attempts due to exception\n",
        "        speak(\"An error occurred. Exiting.\")\n",
        "        break\n",
        "    speak(\"Sorry, I didn't catch that.\")\n",
        "    print(f\"Error: {e}\") # Print the error for debugging"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗣 Hi! I’m your Cooking Assistant. Say 'start' to begin.\n",
            "Enter command: \n",
            "🗣 Sorry, I didn't catch that.\n",
            "Enter command: start\n",
            "🗣 Let's cook Pancakes. Say 'next' when you're ready.\n",
            "Enter command: next\n",
            "🗣 Mix all the dry ingredients in a bowl.\n",
            "Enter command: ok\n",
            "Enter command: next\n",
            "🗣 Add eggs and milk. Stir until smooth.\n",
            "Enter command: next\n",
            "🗣 Heat a skillet and pour 1/4 cup batter.\n",
            "Enter command: next\n",
            "🗣 Flip when bubbles form. Cook until golden.\n",
            "Enter command: next\n",
            "🗣 You're done! Enjoy your meal.\n",
            "Enter command: next\n",
            "🗣 You're done! Enjoy your meal.\n",
            "Enter command: exit\n",
            "🗣 Goodbye, and happy cooking!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb855ee"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   An attempt was made to add a markdown cell explaining the change from voice to text input, but this failed as the `code_block` command is intended for executing Python code, not creating markdown cells.\n",
        "*   The `listen` function was successfully modified to accept text input from the user via the console instead of capturing audio from a microphone. This involved removing code related to audio capture and transcription and implementing the `input()` function.\n",
        "*   Testing the main loop with simulated text input confirmed that the assistant correctly processed and responded to text commands like \"start\", \"next\", and \"exit\", demonstrating that the modification to use text input is working as intended.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   While text input simulation is useful for testing, implementing actual voice input would require addressing the challenges of capturing live audio in the notebook environment and integrating appropriate speech recognition libraries.\n",
        "*   Explore alternative methods or tools within the notebook environment to add markdown cells programmatically if further documentation within the notebook is required.\n"
      ]
    }
  ]
}